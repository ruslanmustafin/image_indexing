{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from utils.imagenet import CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use glob to get a list of images that match a regular expression\n",
    "image_files = sorted(glob.glob('./images/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_f, max_side=512):\n",
    "    img = Image.open(img_f)\n",
    "    img.thumbnail((max_side, max_side), Image.ANTIALIAS)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def embed_image(img, model, emb, prep):\n",
    "    input_tensor = prep(img)\n",
    "    \n",
    "    # create a mini-batch as expected by the model\n",
    "    # unsqueeze will insert a new dimension into \n",
    "    # our tensor\n",
    "    input_batch = input_tensor.unsqueeze(0) \n",
    "    tags = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(input_batch)\n",
    "        e = emb.unit_tensor\n",
    "            \n",
    "    return e[0]\n",
    "\n",
    "class IntermediateTensor(object):\n",
    "    def __init__(self, layer):\n",
    "        _ = layer.register_forward_hook(self.__hook)\n",
    "        self.__tensor = None\n",
    "        \n",
    "    def __hook(self, module, inpt, output):\n",
    "        self.__tensor = output\n",
    "      \n",
    "    @property\n",
    "    def unit_tensor(self):\n",
    "        t_out = self.__tensor.squeeze()\n",
    "        \n",
    "        # preserve batch dim after squeeze\n",
    "        if t_out.ndim==1:\n",
    "            t_out = t_out[None]\n",
    "        \n",
    "        return t_out.renorm_(2, 0, 1)\n",
    "    \n",
    "    @property\n",
    "    def tensor(self):\n",
    "        t_out = self.__tensor.squeeze()\n",
    "        \n",
    "        # preserve batch dim after squeeze\n",
    "        if t_out.ndim==1:\n",
    "            t_out = t_out[None]\n",
    "        \n",
    "        return t_out\n",
    "        \n",
    "        \n",
    "def distance(e1, e2):\n",
    "    \"\"\" Compute euclidian (L2) distance between two unit vectors\n",
    "    \n",
    "    Parameters:\n",
    "        e1 (torch.Tensor): first tenosr\n",
    "        e2 (torch.Tensor): second tenosr\n",
    "    \"\"\"\n",
    "    return 2 * (1 - e1 @ e2.T)\n",
    "\n",
    "\n",
    "class EmbeddingsDatabase(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.table = {}\n",
    "    \n",
    "    def insert(self, key, data):\n",
    "        self.table[key] = data\n",
    "        \n",
    "    def select(self, where, sort=True):\n",
    "        scores = []\n",
    "\n",
    "        # score all entries in a database\n",
    "        for key, emb in self.table.items():\n",
    "            d = distance(emb, where)\n",
    "            scores.append((key, d))\n",
    "            \n",
    "        if sort:\n",
    "            # descending sort \n",
    "            results = sorted(scores, key=lambda x: x[1])\n",
    "        else:\n",
    "            results = scores\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use renset50 model for embedding\n",
    "res50_model = models.resnet50(pretrained=True)\n",
    "res50_model = res50_model.eval()\n",
    "\n",
    "embeddings_op = IntermediateTensor(res50_model.avgpool)\n",
    "\n",
    "# define preprocessing pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    # resize to 224px\n",
    "    transforms.Resize(256),\n",
    "    # put into 0..1 range\n",
    "    transforms.ToTensor(),\n",
    "    # scale into -1 .. 1\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's take a similar and a dissimilar images and compute distance between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_orig = load_image('./images/eiliv-sonas-aceron-gqxSUgngBPA-unsplash.jpg')\n",
    "# content-wise similar image\n",
    "img_similar = load_image('./images/duncan-kidd-Js4jgpksRGk-unsplash.jpg')\n",
    "# content-wise dissimilar image\n",
    "img_dissimilar = load_image('./images/taneli-lahtinen-0cSOFraG4uc-unsplash.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_orig.thumbnail((256, 256))\n",
    "display(img_orig)\n",
    "\n",
    "# img_similar.thumbnail((256, 256))\n",
    "display(img_similar)\n",
    "\n",
    "# img_dissimilar.thumbnail((256, 256))\n",
    "display(img_dissimilar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_orig = embed_image(img_orig, res50_model, embeddings_op, preprocess)\n",
    "e_similar = embed_image(img_similar, res50_model, embeddings_op, preprocess)\n",
    "e_dissimilar = embed_image(img_dissimilar, res50_model, embeddings_op, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('orig, similar', distance(e_orig, e_similar))\n",
    "print('orig, dissimilar', distance(e_orig, e_dissimilar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = EmbeddingsDatabase()\n",
    "\n",
    "database_images = image_files[:-1]\n",
    "\n",
    "for img_f in tqdm.tqdm_notebook(database_images):\n",
    "    img = load_image(img_f)\n",
    "    \n",
    "    e = embed_image(img, res50_model, embeddings_op, preprocess)\n",
    "    \n",
    "    database.insert(img_f, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embs for the second image\n",
    "database.table[database_images[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_img = load_image(image_files[-1])\n",
    "inpt_emb = embed_image(inpt_img, res50_model, embeddings_op, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(inpt_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = database.select(where=inpt_emb)\n",
    "\n",
    "# take top 3 matches\n",
    "results = results[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_f, score in results:\n",
    "    img = load_image(img_f)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyft",
   "language": "python",
   "name": "lyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
