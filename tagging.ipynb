{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from utils.imagenet import CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use glob to get a list of images that match a regular expression\n",
    "image_files = sorted(glob.glob('./images/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize images to be no larger than 512px\n",
    "max_side = 512\n",
    "\n",
    "img = Image.open(image_files[3])\n",
    "img.thumbnail((max_side, max_side), Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use renset50 model for tagging\n",
    "res50_model = models.resnet50(pretrained=True)\n",
    "res50_model = res50_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: Preprocessing\n",
    "\n",
    "Data preprocessing, in the context of image processing, means turning a sample of data into a format, suitable for a given network. This step is necessary to ensure that distribution of pixel in a test image matches the distribution that was used during training. \n",
    "\n",
    "In the case of ResNet50, it was trained on images where each pixel had been shifted and scaled from 0..255 range to *aproximately* -1..1, so we need \n",
    "to put our images into the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these values are computed from imagenet\n",
    "# data samples by taking mean and std along\n",
    "# each RGB channel\n",
    "mean = [0.485, 0.456, 0.406] \n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample 500 points from a normal distribution, shift and re-norm it into 0..255 range and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "\n",
    "# original 0...255 distirbution\n",
    "img_r = (np.random.randn(N)*std[0]*127.5 + 255.0*mean[0])\n",
    "img_g = (np.random.randn(N)*std[1]*127.5 + 255.0*mean[1])\n",
    "img_b = (np.random.randn(N)*std[2]*127.5 + 255.0*mean[2])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.distplot(img_r, color='red')\n",
    "sns.distplot(img_g, color='green')\n",
    "sns.distplot(img_b, color='blue')\n",
    "\n",
    "plt.xlim(0.0, 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's normalize the inputs into -1..1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized -1..1 distribution\n",
    "img_r_n = (img_r/255.0 - mean[0])/std[0]\n",
    "img_g_n = (img_g/255.0 - mean[1])/std[1]\n",
    "img_b_n = (img_b/255.0 - mean[2])/std[2]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "sns.distplot(img_r_n, color='red')\n",
    "sns.distplot(img_g_n, color='green')\n",
    "sns.distplot(img_b_n, color='blue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting back to our task at hand, the same computations need to be performed on our input data. Thankfully, PyTorch (more specifically, torchvision) has a **Transfoms** module that will do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    # resize to 224\n",
    "    transforms.Resize(224),\n",
    "    # put into 0..1 range\n",
    "    transforms.ToTensor(),\n",
    "    # scale into -1 .. 1\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_f, max_side=512):\n",
    "    img = Image.open(img_f)\n",
    "    img.thumbnail((max_side, max_side), Image.ANTIALIAS)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def tags_for_image(img, model, prep, top_k=5):\n",
    "    input_tensor = prep(img)\n",
    "    \n",
    "    # create a mini-batch as expected by the model\n",
    "    # unsqueeze will insert a new dimension into \n",
    "    # our tensor\n",
    "    input_batch = input_tensor.unsqueeze(0) \n",
    "    tags = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        \n",
    "        top_k_inds = probs[0].topk(5).indices.cpu().numpy()\n",
    "        \n",
    "        for ind in top_k_inds:\n",
    "            items = [item.strip() for item in CLASSES[ind].split(',')]\n",
    "            tags.extend(items)\n",
    "            \n",
    "    return tags\n",
    "\n",
    "class TagsDatabase(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.table = {}\n",
    "    \n",
    "    def insert(self, key, data):\n",
    "        self.table[key] = data\n",
    "        \n",
    "    def select(self, where, sort=True):\n",
    "        scores = []\n",
    "\n",
    "        # score all entries in a database\n",
    "        for key, tags in self.table.items():\n",
    "            matches = sum([t in tags for t in where])\n",
    "\n",
    "            scores.append((key, matches))\n",
    "            \n",
    "        if sort:\n",
    "            # descending sort \n",
    "            results = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        else:\n",
    "            results = scores\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_for_image(img, res50_model, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I model the \n",
    "# tags_database = {}\n",
    "database = TagsDatabase()\n",
    "\n",
    "for img_f in tqdm.tqdm_notebook(image_files[:-1]):\n",
    "    img = load_image(img_f)\n",
    "    \n",
    "    tags = tags_for_image(img, res50_model, preprocess)\n",
    "    \n",
    "    # tags_database[img_f] = tags\n",
    "    database.insert(img_f, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags for the second image\n",
    "database.table[image_files[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(load_image(image_files[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_img = load_image(image_files[-1])\n",
    "inpt_tags = tags_for_image(inpt_img, res50_model, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = database.select(where=inpt_tags)\n",
    "\n",
    "# take top 3 matches\n",
    "results = results[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(inpt_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_f, score in results:\n",
    "    img = load_image(img_f)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyft",
   "language": "python",
   "name": "lyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
